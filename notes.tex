\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=2.5cm]{geometry}
\usepackage{xcolor}
\usepackage{siunitx}

\graphicspath{{figures/}}

\title{Notes on the Internal Calibration Model\\for Gaia BP/RP Spectrophotometry}
\author{}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

%----------------------------------------------------------------------
\section{Experimental Data}
\label{sec:data}
%----------------------------------------------------------------------

This section describes the dataset used for calibration experiments.
All data originates from the Gaia BP/RP spectrophotometric pipeline
and corresponds to the INIT data segment.

%----------------------------------------------------------------------
\subsection{Raw Data}
\label{sec:raw-data}

The input data consists of Parquet files containing epoch spectra for
individual sources. Each source may have observations in two
spectrophotometric bands:
\begin{itemize}
  \item \textbf{BP} (Blue Photometer): covering wavelengths
    $330$--$680$~nm,
  \item \textbf{RP} (Red Photometer): covering wavelengths
    $640$--$1050$~nm.
\end{itemize}

Each epoch (a single transit observation of a source) contains:
\begin{itemize}
  \item 60 spectral samples with flux values and associated errors,
  \item 60 reference pseudo-wavelength (PWL) positions,
  \item per-sample validity mask,
  \item metadata: transit ID, CCD row (1--7), field of view
    (Preceding/Following), gate setting, window class (1D/2D),
    across-scan (AC) position, acquisition time (OBMT), $G$ magnitude,
    and source colour.
\end{itemize}

The transit ID encodes observation geometry: the on-board mission
timeline (OBMT), field of view (FoV), CCD row, and AC pixel
position.

The raw dataset contains:
\begin{itemize}
  \item \num{99664} sources,
  \item \num{1318322} epoch spectra (summed over both BP and RP).
\end{itemize}

%----------------------------------------------------------------------
\subsection{Filtering}
\label{sec:filtering}

Several filters are applied during data conversion.  The first three
are metadata-based quality checks that match the operational Java
pipeline; the epoch outlier filter adds data-level checks for
pathological spectra.  Filter configuration is specified in the YAML
configuration file (\texttt{intcal\_test.yaml}).

\subsubsection{Quality Filters}

\begin{enumerate}
  \item \textbf{NominalOnly.} Rejects epochs with non-standard gate
    configurations: specifically, epochs marked as
    \texttt{ComplexGate}, and 1D epochs (\texttt{wc=OneD}) that are
    gated. Only ungated 1D and all 2D epochs pass this filter.

  \item \textbf{UngatedOnly.} Rejects all gated epochs (gate $\neq
    0$), keeping only ungated observations. This is stricter than
    NominalOnly and removes gated 2D epochs as well.

  \item \textbf{FilterHotColumns.} Rejects epochs affected by known
    detector hot columns. For each epoch, the filter checks whether
    any known hot column falls within the epoch's AC window range
    during the epoch's OBMT time range. An epoch is rejected if the
    total hot column strength exceeds a threshold of~1.0. The hot
    column catalogue contains 6~BP and 14~RP entries, each with a
    specific (CCD row, AC position, strength, OBMT range).
\end{enumerate}

\subsubsection{Time Range Filter}

Only epochs within the INIT OBMT range are retained:
\[
  \text{OBMT} \in [122212800000000001,\; 169365600000000000].
\]

\subsubsection{Epoch Outlier Filter}
\label{sec:outlier-filter}

The metadata-based filters above remove epochs with known instrumental
configurations that are unsuitable for calibration.  However, some
epochs that pass all metadata checks still carry \emph{pathological
spectral data}: readout errors that corrupt an entire spectrum, or
cosmic-ray hits that produce isolated extreme-value samples.  Because
the Huber loss (Section~\ref{sec:huber}) \emph{downweights} but does
not \emph{remove} outlying observations, a single catastrophic epoch
can dominate the chi-squared of its source and bias the instrument
model fit.

To address this, two data-level checks are applied to each epoch
during conversion, after the metadata filters and before the
minimum-epochs aggregation:

\begin{enumerate}
  \item \textbf{Negative integrated flux.}  Let $v_1, \ldots, v_n$ be
    the valid spectral samples (those within the PWL range
    $[2.0,\,58.0]$).  If the integrated flux
    $\sum_{j=1}^{n} v_j < 0$,
    the epoch is rejected.  This catches readout errors and electronic
    glitches that produce physically impossible all-negative spectra.
    Example: one epoch of source 897643221358262784 had all 53~valid
    BP samples at ${\approx}{-2745}$~electrons, contributing
    chi2~$\approx 44\,\text{M}$ (accounting for ${\sim}100\%$ of the
    source's total loss).

  \item \textbf{Extreme single-sample spike.}  Let
    $m = \max_j |v_j|$ and
    $\bar{r} = (\sum_j |v_j| - m) / (n - 1)$
    be the mean absolute flux of the remaining $n-1$ samples.  If
    $m > 100\,\bar{r}$,
    the epoch is rejected.  This catches cosmic-ray hits that deposit
    charge in a single detector pixel, producing one sample with a
    value hundreds of times larger than the surrounding spectrum.  The
    threshold of~100 is conservative: a genuine astrophysical feature
    would not produce a single sample exceeding $100\times$ the mean
    of its neighbours.
\end{enumerate}

On the full dataset, this filter rejects 77~epochs out of
\num{409885} that survived the metadata filters: 17 with negative
integrated flux (6~RP, 11~BP) and 60 with extreme spikes (59~BP,
1~RP).  The strong BP predominance is consistent with the higher
cosmic-ray susceptibility of the blue photometer.  Two sources lose
enough epochs to fall below the minimum-epochs threshold and are
discarded entirely.

A JSON sidecar file (\texttt{rejected\_epochs.json}) is written during
conversion with the source~ID, transit~ID, XP~type, CU, rejection
reason, and diagnostic detail for every rejected epoch.

\subsubsection{Minimum Epochs Filter}

After all per-epoch filters are applied (including the outlier
checks), sources with fewer than 20 remaining epochs (per XP type) are
discarded entirely. This ensures that each retained source has
sufficient observations for a reliable spectral fit.

%----------------------------------------------------------------------
\subsection{Valid Spectral Samples}
\label{sec:valid-samples}

Of the 60 raw samples per epoch, only those within the
pseudo-wavelength (PWL) range $[2.0, 58.0]$ are used for
calibration. The number of valid samples depends on the observation
positions (PWL positions) of each epoch, which vary with transit
geometry. For the
reference calibration unit:
\begin{itemize}
  \item BP: 53 valid samples (out of 60),
  \item RP: 54 valid samples (out of 60).
\end{itemize}
Other calibration units may yield 53--59 valid samples per epoch,
depending on their CCD row and field of view.

%----------------------------------------------------------------------
\subsection{Filtered Dataset}
\label{sec:filtered-data}

After applying all filters, the dataset is reduced to:

\begin{table}[h]
\centering
\begin{tabular}{lrr}
  \toprule
  & \textbf{Raw} & \textbf{After filtering} \\
  \midrule
  Sources (total)      & \num{99664} & \num{8557} \\
  Epoch spectra (total) & \num{1318322} & \num{409808} \\
  \midrule
  Sources with BP data & -- & \num{8339} \\
  Sources with RP data & -- & \num{7849} \\
  Sources with both    & -- & \num{7631} \\
  BP-only sources      & -- & \num{708} \\
  RP-only sources      & -- & \num{218} \\
  \midrule
  BP epochs            & -- & \num{211752} \\
  RP epochs            & -- & \num{198056} \\
  \midrule
  BP epochs/source     & -- & 20--67 (median 23) \\
  RP epochs/source     & -- & 20--64 (median 23) \\
  \bottomrule
\end{tabular}
\caption{Dataset size before and after filtering.  Filtering includes
  metadata checks (Section~\ref{sec:filtering}), epoch outlier
  rejection (Section~\ref{sec:outlier-filter}), and a minimum of
  20~epochs per source per XP type.}
\label{tab:data-summary}
\end{table}

The filtering removes ${\sim}91\%$ of sources and ${\sim}69\%$ of
epochs. The dominant factor is the minimum-epochs requirement: most
sources in the raw data have fewer than 20 observations in a single
band within the INIT time range.

%----------------------------------------------------------------------
\subsection{Calibration Units}
\label{sec:cal-units}

Observations are grouped into \emph{calibration units} (CUs) defined
by the combination of:
\begin{itemize}
  \item XP type (BP or RP),
  \item CCD row (1--7),
  \item field of view (Preceding or Following).
\end{itemize}
This gives $2 \times 7 \times 2 = 28$ CUs in total.  One CU per XP
type is designated as the \emph{reference CU} (row~4, FoV~Following),
read from the pipeline configuration.  The treatment of the reference
CU depends on the Aij mode (Section~\ref{sec:aij}):
\begin{itemize}
  \item In \texttt{identity} mode, $\Delta\mathbf{M}^{\text{ref}} =
    \mathbf{0}$ is enforced as a hard gauge constraint, so the
    instrument update solves $2 \times 13 = 26$ non-reference CUs.
  \item In \texttt{per-cu} and \texttt{reference} modes, all $2
    \times 14 = 28$ CUs are solved, including the reference.  The
    Tikhonov regularisation provides a soft anchor (see
    Section~\ref{sec:regularisation}).
\end{itemize}

%----------------------------------------------------------------------
\subsection{Experimental Subsets}
\label{sec:subsets}

For development and validation, we work with subsets of the full
\num{8557}-source dataset. Sources are selected by taking the first
$N$ source IDs (sorted), and all their epochs are included.

\begin{table}[h]
\centering
\begin{tabular}{rrrrrrl}
  \toprule
  \textbf{Sources} & \textbf{Train} & \textbf{Test}
    & \textbf{Test frac.} & \textbf{Train obs} & \textbf{Test obs}
    & \textbf{Purpose} \\
  \midrule
  200  & 160 & 40  & 20\% & \num{408300}  & \num{98879}  & Pilot runs \\
  1000 & 700 & 300 & 30\% & \num{1900814} & \num{814925} & Experiment runs \\
  \bottomrule
\end{tabular}
\caption{Experimental source subsets. Train/test split is random with
  a fixed seed (42) for reproducibility. The test set monitors
  overfitting of the shared instrument model.}
\label{tab:subsets}
\end{table}

The train/test split is used for cross-validation of the instrument
model: both training and test sources receive source updates each
iteration, but only training sources contribute to the instrument
update. A rising test loss relative to training loss would indicate
overfitting of the shared instrument correction~$\Delta M$.

\newpage
%======================================================================
\section{Mathematical Model}
\label{sec:model}
%======================================================================

This section describes the forward model, loss function, and
optimization algorithm used in the calibration.
Table~\ref{tab:notation} summarises the notation.

\begin{table}[h]
\centering
\begin{tabular}{cll}
  \toprule
  \textbf{Symbol} & \textbf{Dimension} & \textbf{Description} \\
  \midrule
  $P$ & 53 (BP), 54 (RP) & Valid output samples per epoch \\
  $K$ & 51 (BP), 54 (RP) & Source basis functions (after PCA) \\
  $G$ & 1361 & Fine grid points in $A_{ij}$ \\
  $N_s$ & varies & Number of epochs for source~$s$ \\
  $S$ & total sources & Number of sources \\
  $C$ & 14 per XP type & Calibration units (13 in identity mode) \\
  \bottomrule
\end{tabular}
\caption{Notation and dimensions.}
\label{tab:notation}
\end{table}

%----------------------------------------------------------------------
\subsection{Forward Model}
\label{sec:forward-model}

\subsubsection{Mean Spectrum}

The mean spectrum of source~$s$ is represented as a linear
combination of $K$ basis functions $B_k(u)$ (B-splines transformed
by a PCA rotation):
\begin{equation}
  \label{eq:mean-spectrum}
  h_s(u) = \sum_{k=1}^{K} b_{s,k}\, B_k(u),
\end{equation}
where $\mathbf{b}_s \in \mathbb{R}^K$ are the source coefficients to
be estimated.  In matrix form, evaluated at the $P$ valid
pseudo-wavelength positions $\mathbf{u}_\text{obs}$:
\begin{equation}
  \mathbf{h}_s = \mathbf{B}\, \mathbf{b}_s,
  \qquad
  B_{ik} = B_k(u_{\text{obs},i}),
  \qquad
  \mathbf{B} \in \mathbb{R}^{P \times K}.
\end{equation}

\subsubsection{Instrument Response: the $A_{ij}$ Matrix}
\label{sec:aij}

The instrument response of each calibration unit is characterised by
its line spread function (LSF) and dispersion relation.  A nominal
instrument model (computed externally from Astrium's numerical LSFs
and the dispersion relations) provides per-CU matrices
$A_{ij}^{\text{nom},c}$ on a fine grid ($\Delta u = 0.05$,
$u \in [-4, 64]$, $G = 1361$ points).  These matrices encode the full
instrument response: how photons at internal wavelength $u_m$ produce
signal at detector position~$u_j$.

For the reference CU, $\mathbf{A}^{\text{nom,ref}}$ encodes the LSF
convolution and dispersion relation of the reference calibration unit
(row~4, FoV~Following).  It is \emph{not} the identity matrix: its
entries are concentrated in a band around the diagonal (diagonal
values ${\sim}0.04$--$0.15$, row sums~${\approx}1$).

\paragraph{Three Aij modes.}
The implementation supports three modes for choosing which $A_{ij}$
matrix is used in the forward model, controlled by the
\texttt{--aij-mode} flag:

\begin{enumerate}
  \item \textbf{\texttt{reference}} (default).  The reference CU's
    matrix $\mathbf{A}^{\text{ref}}$ is used for \emph{all} epochs,
    regardless of which CU observed them.  The
    correction~$\Delta\mathbf{M}^c$ must capture the entire
    differential instrument response between CU~$c$ and the reference.

  \item \textbf{\texttt{per-cu}}.  Each epoch uses the nominal matrix
    $\mathbf{A}^{\text{nom},c}$ of its own CU.  The
    correction~$\Delta\mathbf{M}^c$ only needs to account for small
    residual deviations from the nominal model.

  \item \textbf{\texttt{identity}}.  The identity matrix
    $\mathbf{A} = \mathbf{I}$ is used for all epochs (no LSF model at
    all).  The correction~$\Delta\mathbf{M}^c$ must absorb both the
    full LSF convolution and the CU-to-CU differences.
\end{enumerate}

\noindent
The treatment of the reference CU differs by mode:
\begin{itemize}
  \item In \textbf{identity mode},
    $\Delta\mathbf{M}^{\text{ref}} = \mathbf{0}$ is enforced as a
    hard constraint.  Without a physical prior model, the system has a
    gauge freedom: corrections can be shifted between the reference
    CU's $\Delta\mathbf{M}$ and the source spectra without changing
    the fit.  Fixing the reference CU breaks this degeneracy.
  \item In \textbf{per-cu} and \textbf{reference} modes, all CUs
    including the reference are solved.  The nominal model
    $\mathbf{A}^{\text{nom}}$ already provides a physically motivated
    baseline, and the Tikhonov prior $\mathbf{a}_0 = \mathbf{0}$
    (``no correction needed'') softly anchors each
    $\Delta\mathbf{M}^c$ toward zero
    (Section~\ref{sec:regularisation}).  This matches the Java
    pipeline, which solves for all CUs.
\end{itemize}
Experimental comparison (Section~\ref{sec:aij-comparison})
shows that per-cu and identity modes converge to
essentially the same final loss (${\sim}1\%$ difference), demonstrating
that $\Delta\mathbf{M}$ has enough degrees of freedom to compensate
for any choice of base $A_{ij}$.

\subsubsection{Effective Basis and Predicted Flux}
\label{sec:effective-basis}

The \emph{effective basis}~$\mathbf{E}$ maps source coefficients to
predicted detector signal.  Given the chosen $A_{ij}$ matrix
(Section~\ref{sec:aij}), it is computed in two steps.

\paragraph{Step 1: Fine-grid convolution.}
Evaluate each basis function on the fine grid and convolve with the
$A_{ij}$ matrix:
\begin{equation}
  \label{eq:eff-basis-fine}
  E^\text{fine}_{jk} = \sum_{m=1}^{G} A_{jm}\, B_k(u^\text{fine}_m)\, \Delta u,
  \qquad j = 1, \ldots, G,\quad k = 1, \ldots, K.
\end{equation}
In matrix form: $\mathbf{E}^\text{fine} = \mathbf{A}\, \mathbf{B}^\text{fine}\, \Delta u \in \mathbb{R}^{G \times K}$.

\paragraph{Step 2: Interpolation to observation positions.}
Each epoch has its own observation positions
$\mathbf{u}_\text{obs} \in \mathbb{R}^P$ (which vary with transit
geometry).  The effective basis at these positions is obtained by
linear interpolation from the fine grid:
\begin{equation}
  \label{eq:eff-basis}
  E_{ik} = \operatorname{interp}\!\bigl(E^\text{fine}_{:,k},\;
    u_{\text{obs},i}\bigr),
  \qquad
  \mathbf{E} \in \mathbb{R}^{P \times K}.
\end{equation}

\paragraph{Predicted flux.}
The predicted observation for source~$s$, epoch~$e$ from CU~$c$ is:
\begin{equation}
  \label{eq:prediction-full}
  \mathbf{f}^\text{pred}_{s,e} =
    \bigl(\mathbf{E}_e + \Delta\mathbf{M}^c \mathbf{B}_e\bigr)\,
      \mathbf{b}_s,
\end{equation}
where $\mathbf{E}_e$ and $\mathbf{B}_e$ are evaluated at epoch~$e$'s
observation positions, and $\Delta\mathbf{M}^c$ is the current
instrument correction for CU~$c$.  At iteration~1,
$\Delta\mathbf{M}^c = \mathbf{0}$ for all CUs, so
$\mathbf{f}^\text{pred} = \mathbf{E}_e\, \mathbf{b}_s$.  In identity
mode, $\Delta\mathbf{M}^{\text{ref}} = \mathbf{0}$ throughout all
iterations.

\subsubsection{Step-by-Step: First Iteration vs.\ Subsequent Iterations}
\label{sec:iteration-flow}

To clarify the calculation flow:

\paragraph{Iteration 1 (cold start).}
\begin{enumerate}
  \item All instrument corrections are zero:
    $\Delta\mathbf{M}^c = \mathbf{0}$ for all CUs.
  \item \textbf{Source Update:} For each source~$s$, stack all its
    epochs (from all CUs) and solve the robust regression
    $\mathbf{f} \approx \mathbf{E}\, \mathbf{b}_s$ using only the
    base effective basis~$\mathbf{E}$ (no correction yet).  In per-cu
    mode each CU already has its own $\mathbf{E}_c$ from the nominal
    model $\mathbf{A}^{\text{nom},c}$; in identity and reference modes
    all CUs share the same~$\mathbf{E}$.  Either way, the source
    coefficients represent a compromise fit across all CUs.
  \item \textbf{Instrument Update:} For each CU~$c$ (excluding the
    reference in identity mode), compute residuals
    $r = f - \mathbf{E}\, \mathbf{b}_s$ (using the just-estimated
    source coefficients but no $\Delta\mathbf{M}$).  Solve for
    $\Delta\mathbf{M}^c$ that best explains these residuals.  The
    first $\Delta\mathbf{M}$ captures the dominant instrument effects
    and produces the largest single-iteration loss drop.
\end{enumerate}

\paragraph{Iteration $k > 1$.}
\begin{enumerate}
  \item The instrument corrections $\Delta\mathbf{M}^c$ from the
    previous iteration are available.
  \item \textbf{Source Update:} Each source is now fitted using the
    \emph{corrected} effective basis
    $\mathbf{E}^\text{corr}_e = \mathbf{E}_e + \Delta\mathbf{M}^{c(e)}
    \mathbf{B}_e$.  Epochs from different CUs contribute different
    design matrices (each incorporating its CU's correction).  The
    source coefficients improve because the instrument effects have
    been partially removed.
  \item \textbf{Instrument Update:} For each CU~$c$ (excluding the
    reference in identity mode), residuals are computed from the base
    model (without $\Delta\mathbf{M}$) using the improved source
    coefficients.  Each $\Delta\mathbf{M}^c$ is re-estimated from
    scratch (not incrementally).  Because the source coefficients are
    better, the residuals more cleanly separate instrument effects from
    astrophysical signal, yielding a refined~$\Delta\mathbf{M}^c$.
\end{enumerate}

\noindent
The alternation converges because each half-step is guaranteed to
decrease (or maintain) the total loss (Section~\ref{sec:monotone}).
In practice, ${\sim}90\%$ of the total improvement occurs in the
first 2--3 iterations, with diminishing returns thereafter.

\subsubsection{Instrument Correction}
\label{sec:im-correction}

For calibration unit~$c$, the full prediction with correction is:
\begin{equation}
  \label{eq:prediction}
  \mathbf{f}^\text{pred} = \mathbf{E}\, \mathbf{b}_s
    + \Delta\mathbf{M}^c\, \mathbf{B}\, \mathbf{b}_s
    = \bigl(\mathbf{E} + \Delta\mathbf{M}^c \mathbf{B}\bigr)\, \mathbf{b}_s
    = \mathbf{E}^\text{corr}\, \mathbf{b}_s,
\end{equation}
where $\mathbf{E}^\text{corr} = \mathbf{E} + \Delta\mathbf{M}^c
\mathbf{B}$ is the corrected effective basis.

In identity mode, $\Delta\mathbf{M}^{\text{ref}} = \mathbf{0}$ is
enforced for the reference CU (row~4, FoV~Following) to fix the
gauge.  In per-cu and reference modes, the reference CU receives a
correction like all other CUs; the Tikhonov prior
(Section~\ref{sec:regularisation}) anchors the system instead.

The correction is \emph{linear in~$\mathbf{b}_s$} for fixed
$\Delta\mathbf{M}^c$, so the source update remains a standard linear
regression problem.

\subsubsection{Standardised Residuals}

Given observed spectrum $\mathbf{f}$ with measurement errors
$\boldsymbol{\sigma}$, the standardised residual for sample~$i$ of
epoch~$e$ is:
\begin{equation}
  \label{eq:residual}
  z_i = \frac{f_i - f^\text{pred}_i}{\sigma_i}.
\end{equation}

\paragraph{Notation mapping.}
For reference, our notation maps to the companion article as
follows: our $\mathbf{A}^{\text{nom},c}$ corresponds to the article's
bold~$\mathbf{A}_{ij}$ (the tabulated nominal instrument model); our
correction matrix $\Delta\mathbf{M}^c$ corresponds to the article's
lowercase~$a_{ij}$ (the perturbation model parameterising residual
instrument effects); our source coefficients~$\mathbf{b}_s$ correspond
to the article's~$b_k$; and the effective basis~$\mathbf{E}$ plays the
role of the article's design matrix for the source update.

%----------------------------------------------------------------------
\subsection{Loss Function}
\label{sec:loss}

The global objective to be minimised is:
\begin{equation}
  \label{eq:total-loss}
  \mathcal{L}(\mathbf{b}, \Delta\mathbf{M})
    = \underbrace{
        \sum_{s,e,i} \rho_\delta\!\bigl(z_{s,e,i}\bigr)
      }_{\mathcal{L}_\text{data}}
    + \underbrace{
        \frac{\lambda}{2} \sum_c
          \bigl\|\Delta\mathbf{M}^c\bigr\|_F^2
      }_{\mathcal{L}_\text{reg}},
\end{equation}
where the sum runs over all sources~$s$, their epochs~$e$, and valid
samples~$i$.

\subsubsection{Huber Loss}
\label{sec:huber}

The data fidelity term uses the Huber loss function:
\begin{equation}
  \label{eq:huber}
  \rho_\delta(z) =
  \begin{cases}
    \dfrac{z^2}{2}
      & \text{if } |z| \le \delta, \\[6pt]
    \delta\,|z| - \dfrac{\delta^2}{2}
      & \text{if } |z| > \delta,
  \end{cases}
\end{equation}
with transition parameter $\delta = 1.345$.  This value provides 95\%
asymptotic efficiency relative to ordinary least squares at the normal
distribution while limiting the influence of outliers.

The Huber loss interpolates between quadratic behaviour near zero
(like least squares) and linear growth in the tails (like $L_1$),
making it robust to outlying observations.  It is convex and
differentiable everywhere, with derivative:
\begin{equation}
  \rho_\delta'(z) = \min\!\bigl(|z|, \delta\bigr) \cdot \operatorname{sign}(z)
    = w(z) \cdot z,
  \qquad
  w(z) = \min\!\Bigl(1, \frac{\delta}{|z|}\Bigr).
\end{equation}
The function $w(z)$ serves as the IRLS weight (Section~\ref{sec:irls}).

\subsubsection{Tikhonov Regularisation}
\label{sec:regularisation}

The regularisation term penalises the Frobenius norm of each
instrument correction matrix $\Delta\mathbf{M}^c$:
\begin{equation}
  \label{eq:reg}
  \mathcal{L}_\text{reg}
    = \frac{\lambda}{2} \sum_{c=1}^{C}
        \bigl\|\Delta\mathbf{M}^c\bigr\|_F^2,
  \qquad
  \lambda = 10^4.
\end{equation}
This prevents the instrument model from absorbing noise and
astrophysical signal that properly belongs to the source spectra.  The
choice $\lambda = 10^4$ yields a regularisation penalty that is
${\sim}2\%$ of the total loss, providing effective control without
over-constraining the instrument corrections.

\paragraph{Connection to the Bayesian prior.}
The regularisation~\eqref{eq:reg} is equivalent to a Gaussian prior
on the instrument corrections with mean $\mathbf{a}_0 = \mathbf{0}$
(``no correction on top of the nominal model'') and covariance
$\mathbf{C}_0 = \sigma_0^2 \mathbf{I}$, where $\lambda = 1/\sigma_0^2$.
The prior contribution to the negative log-posterior is
$(\mathbf{a}_0 - \mathbf{a})^\top \mathbf{C}_0^{-1}
(\mathbf{a}_0 - \mathbf{a})
= \lambda \|\mathbf{a}\|^2$,
which matches our $(\lambda/2)\|\Delta\mathbf{M}\|_F^2$ up to the
factor of $1/2$ absorbed into the Huber loss convention.
This makes explicit that the regularisation encodes the physical
assumption that the nominal instrument model is approximately correct.

\textbf{No regularisation is applied to the source coefficients}
$\mathbf{b}_s$.  Source spectra are physical quantities that should
not be biased toward zero, and each source update is well-determined
($N_s \cdot P \gg K$, typically ${\sim}1200$ observations for
${\sim}52$ parameters).

%----------------------------------------------------------------------
\subsection{Alternating Optimisation}
\label{sec:alternating}

The loss~\eqref{eq:total-loss} is minimised by alternating between
two steps, each of which is guaranteed not to increase the total loss.

\subsubsection{Source Update (SU)}
\label{sec:su}

For each source $s$ independently, and for each XP type, solve:
\begin{equation}
  \label{eq:su}
  \min_{\mathbf{b}_s}
    \sum_{e \in \text{epochs}(s)} \sum_i
      \rho_\delta\!\Biggl(
        \frac{f_{e,i} - \bigl(\mathbf{E}^\text{corr}_e\,
          \mathbf{b}_s\bigr)_i}{\sigma_{e,i}}
      \Biggr),
\end{equation}
where $\mathbf{E}^\text{corr}_e = \mathbf{E} +
\Delta\mathbf{M}^{c(e)} \mathbf{B}$, with $c(e)$ denoting the
calibration unit to which epoch~$e$ belongs.
All epochs of source~$s$ are stacked into a single design matrix
$[\mathbf{E}^\text{corr}_{e_1}; \mathbf{E}^\text{corr}_{e_2};
\ldots]$ and solved as one robust regression.

Sources are decoupled in $\mathcal{L}_\text{data}$ and do not appear
in $\mathcal{L}_\text{reg}$, so each source update can be performed
independently.  Both training and test sources are updated.

\subsubsection{Instrument Update (IU)}
\label{sec:iu}

For each CU~$c$ independently (excluding the reference CU in identity
mode), the residuals from the base model (without any
$\Delta\mathbf{M}$) are:
\begin{equation}
  r_{n,p} = f_{n,p} - \bigl(\mathbf{E}\, \mathbf{b}_{s(n)}\bigr)_p,
\end{equation}
where $n$ indexes epochs belonging to CU~$c$ (we use $n$ rather
than~$e$ to distinguish this CU-specific indexing from the per-source
epoch indexing in the SU) and $p$ indexes output samples.  The correction model for these residuals is:
\begin{equation}
  r_{n,p} = \sum_q \Delta M^c_{pq}\, h_{s(n),q} + \text{noise},
\end{equation}
where $\mathbf{h}_{s(n)} = \mathbf{B}\, \mathbf{b}_{s(n)}$ is the
mean spectrum.

\paragraph{Row decoupling.}
A key structural property is that the \emph{rows of
$\Delta\mathbf{M}^c$ decouple}: the unknowns in row~$p$ (i.e.,
$\Delta M^c_{p,:}$) only appear in the equations for sample~$p$.
This allows the $P \times P$ matrix to be solved as $P$ independent
regressions, each with $P$~parameters and $N_c$~epochs:
\begin{equation}
  \label{eq:iu-row}
  \min_{\mathbf{m}_p}
    \sum_{n=1}^{N_c}
      \rho_\delta\!\Biggl(
        \frac{r_{n,p} - \mathbf{h}_{s(n)}^\top \mathbf{m}_p}
             {\sigma_{n,p}}
      \Biggr)
    + \frac{\lambda}{2} \|\mathbf{m}_p\|^2,
\end{equation}
where $\mathbf{m}_p = \Delta\mathbf{M}^c[p, :]$ is the $p$-th row of the correction matrix.  This
gives $P^2$ parameters per CU (e.g., $53^2 = 2809$ for BP), solved
efficiently as $P$ small independent problems.

\textbf{Only training sources contribute to the IU.}  This enables
cross-validation: test sources receive source updates using the
current $\Delta\mathbf{M}$, but their epochs are excluded from the
instrument fit.

The instrument correction $\Delta\mathbf{M}^c$ is re-estimated from
scratch at each iteration (not refined incrementally), since the
residuals $r_{n,p}$ already reflect the latest source coefficients.

%----------------------------------------------------------------------
\subsection{IRLS Solver}
\label{sec:irls}

Both the source and instrument updates are solved via Iteratively
Reweighted Least Squares (IRLS).  The general problem is:
\begin{equation}
  \label{eq:irls-problem}
  \min_{\mathbf{x}}
    \sum_i \rho_\delta\!\bigl(\tilde{b}_i - (\tilde{\mathbf{A}} \mathbf{x})_i\bigr)
    + \frac{\lambda}{2} \|\mathbf{x}\|^2,
\end{equation}
where $\tilde{\mathbf{A}} = \mathbf{A} / \boldsymbol{\sigma}$ and
$\tilde{\mathbf{b}} = \mathbf{b} / \boldsymbol{\sigma}$ are pre-scaled by
measurement errors.  For the source update, $\lambda = 0$.

\paragraph{Algorithm.}
\begin{enumerate}
  \item \textbf{Initialise:} solve the unweighted (OLS) problem
    \[
      \bigl(\tilde{\mathbf{A}}^\top \tilde{\mathbf{A}} + \lambda\,\mathbf{I}\bigr)\,
        \mathbf{x}
      = \tilde{\mathbf{A}}^\top \tilde{\mathbf{b}}.
    \]
  \item \textbf{Iterate} (up to 30 iterations):
    \begin{enumerate}
      \item Compute standardised residuals:
        $z_i = \tilde{b}_i - (\tilde{\mathbf{A}} \mathbf{x})_i$.
      \item Compute IRLS weights:
        $w_i = \min\!\bigl(1,\; \delta / |z_i|\bigr)$.
      \item Solve the weighted normal equations:
        \[
          \bigl(\tilde{\mathbf{A}}^\top \mathbf{W} \tilde{\mathbf{A}}
            + \lambda\,\mathbf{I}\bigr)\, \mathbf{x}_\text{new}
          = \tilde{\mathbf{A}}^\top \mathbf{W}\, \tilde{\mathbf{b}},
        \]
        where $\mathbf{W} = \operatorname{diag}(\mathbf{w})$.
      \item Check convergence:
        $\max_j |x_{\text{new},j} - x_j| < \text{tol} \cdot (1 + \max_j |x_j|)$,
        with $\text{tol} = 10^{-6}$.
    \end{enumerate}
  \item Return $\mathbf{x}$ and the final Huber loss.
\end{enumerate}

The linear system in each IRLS step is solved via Cholesky
factorisation ($\tilde{\mathbf{A}}^\top \mathbf{W} \tilde{\mathbf{A}} +
\lambda\,\mathbf{I}$ is positive definite for $\lambda > 0$).

\paragraph{Convergence.}
Each IRLS step solves a convex quadratic that majorises the Huber
objective at the current~$\mathbf{x}$.  The Huber loss is therefore
non-increasing at every IRLS step, and for $\delta = 1.345$
(close to quadratic), convergence is typically reached in 3--5
iterations.

%----------------------------------------------------------------------
\subsection{Monotonic Decrease Guarantee}
\label{sec:monotone}

Let $\mathcal{L}^k$ denote the total loss at the beginning of
iteration~$k$.  The iteration proceeds as:
\[
  \mathbf{b}^k, \Delta\mathbf{M}^k
  \;\xrightarrow{\text{SU}}\;
  \mathbf{b}^{k+1}, \Delta\mathbf{M}^k
  \;\xrightarrow{\text{IU}}\;
  \mathbf{b}^{k+1}, \Delta\mathbf{M}^{k+1}.
\]

\begin{enumerate}
  \item \textbf{SU:}
    $\mathcal{L}(\mathbf{b}^{k+1}, \Delta\mathbf{M}^k)
      \le \mathcal{L}(\mathbf{b}^k, \Delta\mathbf{M}^k) = \mathcal{L}^k$,
    because each source minimisation can only decrease (or maintain)
    the loss.

  \item \textbf{IU:}
    $\mathcal{L}(\mathbf{b}^{k+1}, \Delta\mathbf{M}^{k+1})
      \le \mathcal{L}(\mathbf{b}^{k+1}, \Delta\mathbf{M}^k)$,
    because each CU minimisation can only decrease (or maintain) the
    loss.
\end{enumerate}

Combining: $\mathcal{L}^{k+1} \le \mathcal{L}^k$.  Since
$\mathcal{L} \ge 0$, the sequence converges by the monotone
convergence theorem.

\paragraph{Per-source caveat.}
The monotonicity guarantee applies to the \emph{total} loss, not to
individual sources.  After the IU, individual sources may see a
per-source loss increase because $\Delta\mathbf{M}^c$ is shared
across all sources in a CU.  The subsequent SU always recovers all
sources.

%----------------------------------------------------------------------
\subsection{Default Parameters}
\label{sec:parameters}

\begin{table}[h]
\centering
\begin{tabular}{llll}
  \toprule
  \textbf{Parameter} & \textbf{Symbol} & \textbf{Value} & \textbf{Description} \\
  \midrule
  Huber threshold & $\delta$ & 1.345 & 95\% efficiency at the normal \\
  Regularisation strength & $\lambda$ & $10^4$ & Tikhonov penalty for $\Delta\mathbf{M}$ \\
  IRLS max iterations & -- & 30 & Per subproblem \\
  IRLS tolerance & -- & $10^{-6}$ & Relative coefficient change \\
  Max outer iterations & -- & 10--15 & SU/IU alternation cycles \\
  \bottomrule
\end{tabular}
\caption{Default calibration parameters.}
\label{tab:parameters}
\end{table}

\newpage
%======================================================================
\section{Experimental Results: Aij Mode Comparison}
\label{sec:aij-comparison}
%======================================================================

We compare the \texttt{per-cu} and \texttt{identity} Aij modes to
assess the importance of the nominal LSF model and the effect of the
reference CU treatment.  The primary runs use 1000~sources
(700~train, 300~test, 30\% test fraction), 15~iterations,
$\delta = 1.345$, $\lambda = 10^4$, seed~42, and the corrected
reference CU (row~4, FoV~Following, read from config).  After
filtering, 964~BP and 932~RP sources remain, with
\num{1900814}~training and \num{814925}~test observations.  A
200-source pilot run (Section~\ref{sec:scaling}) confirms that the
findings are stable across sample sizes.

The key implementation difference between modes:
\begin{itemize}
  \item \textbf{per-cu}: each CU uses its own nominal
    $\mathbf{A}^{\text{nom},c}$; all 28~CUs ($2 \times 14$) are
    solved, including the reference.  The Tikhonov prior anchors the
    system.
  \item \textbf{identity}: $\mathbf{A} = \mathbf{I}$ for all CUs;
    the reference CU is \emph{excluded} from the instrument update
    ($\Delta\mathbf{M}^{\text{ref}} = \mathbf{0}$) to fix the gauge,
    yielding 26~solved CUs.
\end{itemize}

%----------------------------------------------------------------------
\subsection{Summary Comparison}

\begin{table}[ht]
\centering
\begin{tabular}{lcc}
  \toprule
  & \textbf{per-cu} & \textbf{identity} \\
  \midrule
  CUs solved            & 28 (all)     & 26 (ref skipped)   \\
  Iterations            & 14 (converged) & 15                \\
  Loss after iter 1     & $1.268 \times 10^6$ & $1.317 \times 10^6$ \\
  Final loss            & $1.161 \times 10^6$ & $1.174 \times 10^6$ \\
  Total loss reduction  & $-8.4\%$     & $-10.8\%$    \\
  \midrule
  Train data loss       & $7.556 \times 10^5$ & $7.654 \times 10^5$ \\
  Test data loss        & $3.961 \times 10^5$ & $3.987 \times 10^5$ \\
  Train loss/obs        & 0.3975       & 0.4027       \\
  Test loss/obs         & 0.4861       & 0.4892       \\
  Test/train ratio      & 1.223        & 1.215        \\
  \midrule
  Reg.\ penalty (final) & $9.651 \times 10^3$ & $1.018 \times 10^4$ \\
  \midrule
  BP sources increased  & 0/964        & 0/964        \\
  RP sources increased  & 0/932        & 0/932        \\
  \midrule
  Residual std          & 9.181        & 9.178        \\
  Residual MAD          & 2.000        & 2.000        \\
  $|r| > 5$             & 24.89\%      & 24.89\%      \\
  \bottomrule
\end{tabular}
\caption{Comparison of per-cu and identity Aij modes (1000 sources,
  ref CU = row~4 FoV~Following).  Per-cu converged early at
  iteration~14 (relative loss change $< 10^{-4}$); identity ran
  the full 15~iterations.  Per-cu achieves
  ${\sim}1\%$ lower total loss by solving all 28~CUs with a nominal
  Aij baseline; residual statistics are identical.}
\label{tab:aij-comparison}
\end{table}

%----------------------------------------------------------------------
\subsection{Convergence Behaviour}

Both modes exhibit the same convergence pattern: a large loss drop in
the first iteration (the initial instrument update captures the
dominant CU-to-CU differences), rapid improvement through
iterations~2--4, and diminishing returns thereafter.  By iteration~10,
the relative change per iteration falls below $0.05\%$ in both modes.

\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \toprule
  \textbf{Iter} & \textbf{per-cu} & \textbf{identity} \\
  \midrule
  1  & $1.268 \times 10^6$ & $1.317 \times 10^6$ \\
  2  & $1.193 \times 10^6$ & $1.212 \times 10^6$ \\
  5  & $1.167 \times 10^6$ & $1.182 \times 10^6$ \\
  10 & $1.162 \times 10^6$ & $1.176 \times 10^6$ \\
  \bottomrule
\end{tabular}
\caption{Total loss after IU at selected iterations (1000 sources).}
\label{tab:convergence}
\end{table}

The per-cu mode achieves consistently lower loss because: (1)~the
nominal $\mathbf{A}^{\text{nom},c}$ already captures most of the
instrument response, so $\Delta\mathbf{M}$ need only model small
residual deviations; (2)~all 28~CUs are corrected, including the
reference CU, whereas identity mode fixes
$\Delta\mathbf{M}^{\text{ref}} = \mathbf{0}$.  Despite the
${\sim}1\%$ total loss gap, the two modes converge at the same rate,
with ${\sim}90\%$ of improvement in the first 3--4 iterations.

%----------------------------------------------------------------------
\subsection{Train/Test Generalisation}

No overfitting is observed in either mode.  Both training and test
data losses decrease monotonically through all iterations.  The
test/train ratio stabilises at ${\sim}1.22$ in both modes, higher than
the ${\sim}1.08$ observed in 200-source runs.  The increase is
attributable to the larger test fraction (30\% vs 20\%) and the
greater diversity of sources: the 1000-source sample includes fainter
and more challenging sources not present in the 200-source subset.
The identity mode has a marginally \emph{lower} test/train ratio
($1.215$ vs $1.223$), because the gauge constraint
($\Delta\mathbf{M}^{\text{ref}} = \mathbf{0}$) acts as an additional
implicit regulariser.

%----------------------------------------------------------------------
\subsection{Regularisation Penalty}

The regularisation penalty $\mathcal{L}_\text{reg}$ reflects the
magnitude of the learned instrument corrections:

\begin{itemize}
  \item \textbf{per-cu} ($9.65 \times 10^3$): 28~CUs contribute,
    each with a small $\Delta\mathbf{M}^c$ (corrections on top of the
    nominal model).  The per-CU penalty is $9651/28 = 345$.
  \item \textbf{identity} ($1.02 \times 10^4$): 26~CUs contribute
    (reference excluded), each encoding the full LSF convolution.
    The per-CU penalty is $10185/26 = 392$, confirming that individual
    corrections are ${\sim}14\%$ larger without a nominal model
    baseline.
\end{itemize}

\noindent
The regularisation penalty has roughly doubled compared to the
200-source runs (${\sim}4.7 \times 10^3$), reflecting the
$5{\times}$ increase in training data: more observations constrain
$\Delta\mathbf{M}$ more tightly, allowing larger corrections before
the regularisation--data tradeoff tightens.  As a fraction of total
loss, the penalty remains modest (${\sim}0.8\%$).

%----------------------------------------------------------------------
\subsection{Reference CU Treatment}

The treatment of the reference CU is the primary structural
difference between modes and explains the ${\sim}1\%$ loss gap:

\begin{itemize}
  \item In \textbf{identity mode}, the reference CU receives no
    correction.  All observations from CU (row~4, FoV~Following)
    are fitted using only the base effective basis~$\mathbf{E}$
    (which, with $\mathbf{A} = \mathbf{I}$, is a raw B-spline
    evaluation without any LSF convolution).  This is the price
    paid for gauge-fixing when no physical prior model is available.

  \item In \textbf{per-cu mode}, every CU---including the
    reference---receives a $\Delta\mathbf{M}^c$ correction.  The
    nominal $\mathbf{A}^{\text{nom},c}$ already encodes the physical
    instrument response, and the Tikhonov prior
    (Section~\ref{sec:regularisation}) anchors the corrections toward
    zero.  The reference CU is not special.
\end{itemize}

\noindent
The per-CU approach matches the Java pipeline, which solves for
all CUs without excluding the reference.

%----------------------------------------------------------------------
\subsection{Scaling: 200 vs 1000 Sources}
\label{sec:scaling}

\begin{table}[ht]
\centering
\begin{tabular}{lcccc}
  \toprule
  & \multicolumn{2}{c}{\textbf{200 sources}} &
    \multicolumn{2}{c}{\textbf{1000 sources}} \\
  \cmidrule(lr){2-3} \cmidrule(lr){4-5}
  & per-cu & identity & per-cu & identity \\
  \midrule
  Sources (BP / RP)     & 187 / 182 & 187 / 182 & 964 / 932 & 964 / 932 \\
  Train observations    & \num{408300} & \num{408300} & \num{1900814} & \num{1900814} \\
  Test fraction         & 20\%    & 20\%    & 30\%    & 30\%    \\
  Train loss/obs        & 0.359   & 0.367   & 0.398   & 0.403   \\
  Test loss/obs         & 0.390   & 0.394   & 0.486   & 0.489   \\
  Test/train ratio      & 1.087   & 1.073   & 1.223   & 1.215   \\
  Residual MAD          & 2.046   & 2.046   & 2.000   & 2.000   \\
  $|r| > 5$             & 25.0\%  & 25.0\%  & 24.9\%  & 24.9\%  \\
  \bottomrule
\end{tabular}
\caption{Scaling comparison.  Residual quality improves slightly with
  more sources despite higher loss/obs values.}
\label{tab:scaling}
\end{table}

\noindent
Two trends emerge:
\begin{enumerate}
  \item \textbf{Residual quality improves.}  The MAD drops from
    $2.046$ to $2.000$ ($-2.2\%$) and the fraction of large residuals
    ($|r| > 5$) drops from $25.0\%$ to $24.9\%$.  More sources
    provide better constraints on $\Delta\mathbf{M}$, yielding
    tighter residuals.
  \item \textbf{Per-observation loss increases.}  The train loss/obs
    rises from ${\sim}0.36$ to ${\sim}0.40$.  This is expected: the
    1000-source sample includes fainter, noisier sources with
    intrinsically higher Huber loss per observation.  The instrument
    model (with ${\sim}2800$ parameters per CU) is shared across all
    sources and cannot simultaneously optimise for a more diverse
    population.
\end{enumerate}

\noindent
The per-cu/identity comparison is robust to the $5{\times}$ sample
size increase: the loss gap between modes (${\sim}1\%$), the identical
residual statistics, and the absence of chi2 increases all hold at
both scales.

%----------------------------------------------------------------------
\subsection{Key Observations}

\begin{enumerate}
  \item \textbf{Per-cu mode achieves the lowest total loss}
    ($1.161 \times 10^6$) by solving all 28~CUs with a physically
    motivated nominal model.  The ${\sim}1\%$ advantage over identity
    mode comes primarily from correcting the reference CU and
    providing a better Aij baseline.

  \item \textbf{Residual statistics are identical.}  Both
    modes produce std~$\approx 9.18$, MAD~$= 2.000$, and
    $|r| > 5 = 24.89\%$.  Zero sources show increased chi2 in either
    mode.  The loss gap is spread uniformly across all sources and
    samples rather than concentrated in specific outliers.

  \item \textbf{$\Delta\mathbf{M}$ has ample capacity.}  With
    $P^2 \approx 2800$ free parameters per CU, the instrument
    correction matrix can absorb the entire LSF convolution
    (${\sim}70$ non-negligible entries per row of $A_{ij}$) while
    still maintaining effective regularisation.  The regularisation
    penalty remains below $1\%$ of the total loss.

  \item \textbf{The Aij choice affects interpretation, not residual
    quality.}  In per-cu mode, $\Delta\mathbf{M}$ represents small
    corrections on top of a known instrument model.  In identity mode,
    $\Delta\mathbf{M}$ encodes the full instrument response including
    the LSF.  The calibrated spectra are indistinguishable.

  \item \textbf{No overfitting at either scale.}  Both training and
    test losses decrease monotonically in all runs.  The higher
    test/train ratio at 1000~sources (${\sim}1.22$ vs ${\sim}1.08$)
    reflects population heterogeneity rather than overfitting: the
    test set contains sources that differ more from the training set.

  \item \textbf{Identity mode requires gauge-fixing.}  Without a
    physical prior, the reference CU must be excluded from the
    instrument update to break the degeneracy between source spectra
    and instrument model.  This is a direct consequence of the
    article's analysis (Section~2.3): when $\mathbf{A} = \mathbf{I}$,
    there is no anchor to distinguish ``instrument effect'' from
    ``source spectrum.''

  \item \textbf{Epoch outlier filter removes pathological data.}
    Investigation of source 897643221358262784 --- previously the worst
    outlier with BP~chi2/obs~$\approx 44$ --- revealed that a single
    corrupted epoch (all 53~valid samples at ${\approx}{-2745}$\,e$^-$,
    a readout error) accounted for ${\sim}100\%$ of the anomalous loss.
    The star itself is an ordinary faint K/G-dwarf ($G = 15.7$,
    $T_\text{eff} \approx 5900$\,K).
    The epoch outlier filter (Section~\ref{sec:outlier-filter}) now
    removes such epochs at conversion time: 77~pathological epochs are
    rejected across the full dataset (17~negative-flux readout errors,
    60~cosmic-ray spikes).
\end{enumerate}

For production use, \texttt{per-cu} mode is recommended: it uses the
full nominal instrument model, solves all CUs without gauge-fixing
complications, and achieves the lowest loss.  The \texttt{identity}
mode serves as a useful validation: its convergence to identical
residual quality confirms that the instrument correction has enough
degrees of freedom to learn the complete instrument response from
data alone.

%----------------------------------------------------------------------
\subsection{200-Source Pilot Run: Detailed Results}
\label{sec:200-results}

This section presents convergence tables and diagnostic figures from
the 200-source, 10-iteration pilot runs (per-cu and identity modes).
The dataset contains \num{409808}~epochs after all quality filters and
epoch outlier removal (Section~\ref{sec:outlier-filter}), split into
160~training and 40~test sources (20\% test fraction, seed~42).
After source-level filtering, 187~BP and 182~RP sources remain,
yielding \num{408300}~training and \num{98879}~test observations.

\subsubsection{Convergence Tables}

Table~\ref{tab:200-convergence-percu} shows the full iteration-by-iteration
convergence for per-cu mode; Table~\ref{tab:200-convergence-identity}
shows identity mode.

\begin{table}[ht]
\centering
\begin{tabular}{rcccc}
  \toprule
  \textbf{Iter} & \textbf{Begin} & \textbf{After SU} & \textbf{After IU}
    & \textbf{Rel.\ change} \\
  \midrule
  1  & ---                   & $4.182 \times 10^5$ & $2.103 \times 10^5$ & --- \\
  2  & $2.103 \times 10^5$   & $1.978 \times 10^5$ & $1.954 \times 10^5$ & $-7.08\%$ \\
  3  & $1.954 \times 10^5$   & $1.929 \times 10^5$ & $1.924 \times 10^5$ & $-1.51\%$ \\
  4  & $1.924 \times 10^5$   & $1.913 \times 10^5$ & $1.912 \times 10^5$ & $-0.61\%$ \\
  5  & $1.912 \times 10^5$   & $1.906 \times 10^5$ & $1.907 \times 10^5$ & $-0.31\%$ \\
  6  & $1.907 \times 10^5$   & $1.902 \times 10^5$ & $1.903 \times 10^5$ & $-0.17\%$ \\
  7  & $1.903 \times 10^5$   & $1.900 \times 10^5$ & $1.901 \times 10^5$ & $-0.10\%$ \\
  8  & $1.901 \times 10^5$   & $1.898 \times 10^5$ & $1.900 \times 10^5$ & $-0.07\%$ \\
  9  & $1.900 \times 10^5$   & $1.897 \times 10^5$ & $1.899 \times 10^5$ & $-0.05\%$ \\
  10 & $1.899 \times 10^5$   & $1.897 \times 10^5$ & $1.898 \times 10^5$ & $-0.04\%$ \\
  \bottomrule
\end{tabular}
\caption{Total loss at each stage of per-cu mode (200 sources,
  10 iterations).  Loss reduction: $2.103 \times 10^5 \to
  1.898 \times 10^5$ ($-9.7\%$).  From iteration~5 onward, the IU
  step slightly increases total loss relative to after~SU because the
  regularisation penalty growth exceeds the marginal data loss
  improvement.}
\label{tab:200-convergence-percu}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{rcccc}
  \toprule
  \textbf{Iter} & \textbf{Begin} & \textbf{After SU} & \textbf{After IU}
    & \textbf{Rel.\ change} \\
  \midrule
  1  & ---                   & $4.794 \times 10^5$ & $2.238 \times 10^5$ & --- \\
  2  & $2.238 \times 10^5$   & $2.049 \times 10^5$ & $2.011 \times 10^5$ & $-10.15\%$ \\
  3  & $2.011 \times 10^5$   & $1.981 \times 10^5$ & $1.973 \times 10^5$ & $-1.87\%$ \\
  4  & $1.973 \times 10^5$   & $1.961 \times 10^5$ & $1.958 \times 10^5$ & $-0.75\%$ \\
  5  & $1.958 \times 10^5$   & $1.951 \times 10^5$ & $1.950 \times 10^5$ & $-0.42\%$ \\
  6  & $1.950 \times 10^5$   & $1.945 \times 10^5$ & $1.945 \times 10^5$ & $-0.26\%$ \\
  7  & $1.945 \times 10^5$   & $1.941 \times 10^5$ & $1.942 \times 10^5$ & $-0.18\%$ \\
  8  & $1.942 \times 10^5$   & $1.939 \times 10^5$ & $1.939 \times 10^5$ & $-0.12\%$ \\
  9  & $1.939 \times 10^5$   & $1.937 \times 10^5$ & $1.938 \times 10^5$ & $-0.09\%$ \\
  10 & $1.938 \times 10^5$   & $1.936 \times 10^5$ & $1.936 \times 10^5$ & $-0.07\%$ \\
  \bottomrule
\end{tabular}
\caption{Total loss at each stage of identity mode (200 sources,
  10 iterations).  Loss reduction: $2.238 \times 10^5 \to
  1.936 \times 10^5$ ($-13.5\%$).  The larger percentage reduction
  compared to per-cu reflects the higher initial loss (no nominal
  model baseline).  The IU step becomes loss-increasing from
  iteration~7 onward.}
\label{tab:200-convergence-identity}
\end{table}

\subsubsection{Train/Test Generalisation}

Table~\ref{tab:200-traintest} shows train/test loss decomposition at
selected iterations for both modes.

\begin{table}[ht]
\centering
\begin{tabular}{lccccc}
  \toprule
  \textbf{Mode / Iter} & \textbf{Reg IM} & \textbf{Train data}
    & \textbf{Test data} & \textbf{Train/obs} & \textbf{Test/obs} \\
  \midrule
  \multicolumn{6}{l}{\textit{per-cu}} \\
  \quad 1  & $3.082 \times 10^3$ & $1.635 \times 10^5$ & $4.365 \times 10^4$ & 0.401 & 0.441 \\
  \quad 5  & $4.399 \times 10^3$ & $1.476 \times 10^5$ & $3.863 \times 10^4$ & 0.362 & 0.391 \\
  \quad 10 & $4.787 \times 10^3$ & $1.465 \times 10^5$ & $3.855 \times 10^4$ & 0.359 & 0.390 \\
  \midrule
  \multicolumn{6}{l}{\textit{identity}} \\
  \quad 1  & $3.209 \times 10^3$ & $1.749 \times 10^5$ & $4.566 \times 10^4$ & 0.429 & 0.462 \\
  \quad 5  & $4.437 \times 10^3$ & $1.514 \times 10^5$ & $3.915 \times 10^4$ & 0.371 & 0.396 \\
  \quad 10 & $4.685 \times 10^3$ & $1.500 \times 10^5$ & $3.897 \times 10^4$ & 0.367 & 0.394 \\
  \bottomrule
\end{tabular}
\caption{Train/test loss decomposition (200 sources).  Both modes
  show monotonically decreasing train and test losses.  Test/train
  ratio at convergence: 1.09 (per-cu) and 1.07 (identity) --- no
  overfitting.  Train: \num{408300} obs, test: \num{98879} obs.}
\label{tab:200-traintest}
\end{table}

\subsubsection{Per-Source Chi2 Summary}

Zero sources show increased overall chi2/obs in either mode (0/187~BP,
0/182~RP).  The worst source is 421398311066127104 with
chi2/obs~$= 0.865$ (BP, per-cu) and $0.702$ (RP, per-cu) --- entirely
normal.

\subsubsection{Convergence Figures}

Figure~\ref{fig:200-convergence-percu} shows the four-panel
convergence diagnostic for per-cu mode.
Figure~\ref{fig:200-convergence-identity} shows the same for identity
mode.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\textwidth]{fig_200_percu_convergence}
  \caption{Convergence diagnostic for per-cu mode (200 sources,
    10 iterations).  Top left: total loss decomposed into data loss and
    regularisation penalty.  Top right: BP and RP data loss.  Bottom
    left: per-source loss/obs distribution (median, 95th percentile,
    max).  Bottom right: relative change per iteration.}
  \label{fig:200-convergence-percu}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\textwidth]{fig_200_identity_convergence}
  \caption{Convergence diagnostic for identity mode (200 sources,
    10 iterations).  Same layout as Figure~\ref{fig:200-convergence-percu}.
    Identity mode starts from a higher initial loss but follows the same
    convergence pattern, with ${\sim}90\%$ of the improvement in the first
    3--4 iterations.}
  \label{fig:200-convergence-identity}
\end{figure}

\subsubsection{Train/Test Generalisation Figure}

Figure~\ref{fig:200-traintest} shows the train/test convergence
diagnostic for per-cu mode.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\textwidth]{fig_200_percu_traintest}
  \caption{Train/test generalisation for per-cu mode (200 sources).
    Top left: absolute data loss for train and test sets.  Top right:
    normalised loss per observation.  Bottom left: train loss
    decomposed into data loss and regularisation penalty.  Bottom
    right: test/train ratio (overfitting indicator); the ratio
    stabilises at ${\sim}1.09$, confirming no overfitting.}
  \label{fig:200-traintest}
\end{figure}

\subsubsection{IU Loss Increase Behaviour}

Figure~\ref{fig:200-iu-increases} shows the fraction of sources whose
per-source loss increases after the instrument update step.  This
fraction grows over iterations as the regularisation penalty
increasingly competes with diminishing data loss improvements.  At
iteration~10, 73\% of BP and 68\% of RP sources experience a
per-source loss increase after IU.  However, the \emph{overall}
per-source chi2 (first iteration vs final) still decreases for all
369~sources.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{fig_200_percu_iu_increases}
  \caption{Fraction of sources with per-source loss increase after the
    instrument update step, per-cu mode (200 sources).  The increase
    reflects the growing tension between regularisation and diminishing
    data loss improvements near convergence.  Despite these per-step
    increases, the overall per-source chi2 decreases for all sources.}
  \label{fig:200-iu-increases}
\end{figure}

\subsubsection{Residual Diagnostics}

Figure~\ref{fig:200-residuals-before-after} compares residuals before
and after calibration.  Figure~\ref{fig:200-residuals-bprp} shows the
BP and RP residual structure by CU row.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\textwidth]{fig_200_percu_residuals_before_after}
  \caption{Residuals before calibration (top row, no $\Delta\mathbf{M}$)
    and after calibration (bottom row, with $\Delta\mathbf{M}$) for
    per-cu mode (200 sources).  Left: residuals vs PWL sample index.
    Centre: residuals vs AC window centre.  Right: residuals vs OBMT
    (time).  The systematic CU-dependent structure visible before
    calibration (coloured curves) is substantially reduced after the
    instrument correction.  MAD improves from 2.190 to 2.046; the
    fraction of large residuals ($|r| > 5$) drops from 25.8\% to
    25.0\%.}
  \label{fig:200-residuals-before-after}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\textwidth]{fig_200_percu_residuals_bprp}
  \caption{Post-calibration residuals for BP (top) and RP (bottom),
    per-cu mode (200 sources).  Left: normalised residuals vs PWL
    sample index, coloured by CU row.  Right: residuals vs AC window
    centre.  The remaining systematic structure in the PWL residuals
    (coloured curves deviating from the flat baseline) indicates
    residual instrument effects not yet captured by
    $\Delta\mathbf{M}$.}
  \label{fig:200-residuals-bprp}
\end{figure}

\subsubsection{Individual Source Example}

Figures~\ref{fig:200-source-percu} and~\ref{fig:200-source-identity}
show the calibration result for a single well-observed source
(283613187526576896, 49~BP epochs, 46~RP epochs) in per-cu and
identity modes respectively.  Each figure has four rows:
\begin{enumerate}
  \item Observed epoch data (coloured by CCD row) and fitted mean
    spectrum (black curve) in linear scale.
  \item Same in log scale, highlighting CU-dependent structure in the
    faint wings.
  \item Raw residuals (observation $-$ model).
  \item Normalised residuals (residual / error).
\end{enumerate}

\noindent
The two modes produce visually indistinguishable fits for this source,
consistent with the aggregate finding that the calibrated spectra are
equivalent despite different Aij baselines
(Section~\ref{sec:aij-comparison}).  The CU-dependent scatter visible
in the log-scale panels (row~2) --- where different CCD rows produce
slightly different observed flux levels --- is absorbed by the
per-epoch $\Delta\mathbf{M}^c$ correction.  The normalised residuals
(row~4) are approximately flat with no visible systematic structure.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\textwidth]{fig_200_percu_source_283613187526576896}
  \caption{Individual source 283613187526576896 --- per-cu mode.  Left
    column: BP (49 epochs).  Right column: RP (46 epochs).  Rows from
    top: flux (linear), flux (log), raw residuals, normalised
    residuals.  Dots are coloured by CCD row (1--7).  The black curve
    is the calibrated mean spectrum $\mathbf{E} \cdot \mathbf{b}$.}
  \label{fig:200-source-percu}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\textwidth]{fig_200_identity_source_283613187526576896}
  \caption{Individual source 283613187526576896 --- identity mode.
    Same layout as Figure~\ref{fig:200-source-percu}.  The fit is
    visually indistinguishable from per-cu mode, confirming that the
    $\Delta\mathbf{M}$ correction absorbs the full instrument response
    when $\mathbf{A} = \mathbf{I}$.}
  \label{fig:200-source-identity}
\end{figure}

\end{document}
